{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\spaces\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fcnvgg import FCNVGG\n",
    "from utils  import *\n",
    "from glob   import glob\n",
    "from tqdm   import tqdm\n",
    "\n",
    "# Imports from demo.py in Udacity Workspace\n",
    "import sys, skvideo.io, json, base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io  import BytesIO, StringIO\n",
    "\n",
    "%load_ext autotime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17 ms\n"
     ]
    }
   ],
   "source": [
    "def sample_generator(video, image_size=(416,320), batch_size=5):\n",
    "    for offset in range(0, len(video), batch_size):\n",
    "        files = video[offset:offset+batch_size]\n",
    "        images = []\n",
    "        for frame in video:\n",
    "            image = cv2.resize(frame, image_size)\n",
    "            images.append(image.astype(np.float32))\n",
    "            #names.append(os.path.basename(image_file))\n",
    "        yield np.array(images)\n",
    "\n",
    "def encode(array):\n",
    "\tpil_img = Image.fromarray(array)\n",
    "\tbuff = BytesIO()\n",
    "\tpil_img.save(buff, format=\"PNG\")\n",
    "\treturn base64.b64encode(buff.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Generate data based on a model')\n",
    "parser.add_argument('--name', default='test',\n",
    "                    help='project name')\n",
    "parser.add_argument('--checkpoint', type=int, default=-1,\n",
    "                    help='checkpoint to restore; -1 is the most recent')\n",
    "parser.add_argument('--video-file', default='test',\n",
    "                    help='video file to analyze')\n",
    "parser.add_argument('--output-dir', default='test-output',\n",
    "                    help='directory for the resulting images')\n",
    "parser.add_argument('--batch-size', type=int, default=10,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--data-source', default='carla',\n",
    "                    help='data source')\n",
    "\n",
    "args = parser\n",
    "args.name        = 'runs\\\\t3'\n",
    "args.checkpoint  = -1\n",
    "args.video_file  = 'test_video.mp4'\n",
    "args.output_dir  = 'test_output'\n",
    "args.batch_size  = 1\n",
    "args.data_source = 'carla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: runs\\t3\\final.ckpt\n",
      "Loaded metagraph:  runs\\t3\\final.ckpt.meta\n",
      "time: 26 ms\n"
     ]
    }
   ],
   "source": [
    "state = tf.train.get_checkpoint_state(args.name)\n",
    "if state is None:\n",
    "    print('[!] No network state found in ' + args.name)\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    checkpoint_file = state.all_model_checkpoint_paths[args.checkpoint]\n",
    "    print(\"Loaded checkpoint: {0}\".format(checkpoint_file))\n",
    "except IndexError:\n",
    "    print('[!] Cannot find checkpoint ' + str(args.checkpoint_file))\n",
    "    sys.exit(1)\n",
    "\n",
    "metagraph_file = checkpoint_file + '.meta'\n",
    "\n",
    "if not os.path.exists(metagraph_file):\n",
    "    print('[!] Cannot find metagraph ' + metagraph_file)\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print('Loaded metagraph:  {0}'.format(metagraph_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    source       = load_data_source(args.data_source)\n",
    "    label_colors = source.label_colors\n",
    "except (ImportError, AttributeError, RuntimeError) as e:\n",
    "    print('[!] Unable to load data source:', str(e))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "video = skvideo.io.vread(args.video_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 320)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "np.shape(video)\n",
    "source.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Creating the model...\n",
      "INFO:tensorflow:Restoring parameters from runs\\t3\\final.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[i] Processing video:  23%|█████████████████████▉                                                                           | 7/31 [00:10<00:35,  1.48s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('[i] Creating the model...')\n",
    "    net = FCNVGG(sess)\n",
    "    net.build_from_metagraph(metagraph_file, checkpoint_file)\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Process the images\n",
    "    #---------------------------------------------------------------------------\n",
    "    #generator = sample_generator(video, source.image_size, args.batch_size)\n",
    "    generator = sample_generator(video)\n",
    "\n",
    "    n_sample_batches = int(math.ceil(len(video)/args.batch_size))\n",
    "    description = '[i] Processing video'\n",
    "\n",
    "    for x in tqdm(generator, total=n_sample_batches, desc=description, unit='batches'):\n",
    "        feed = {net.image_input:  x,\n",
    "                net.keep_prob:    1}\n",
    "        img_labels = sess.run(net.classes, feed_dict=feed)\n",
    "\n",
    "        #binary_car_result = np.where()\n",
    "        imgs = draw_labels_batch(x, img_labels, label_colors, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
